

📘 NLP 相关 
1. 预处理 (Preprocessing)
  a. 分词 (tokenization)
     中文：常用结巴分词、BERT分词（WordPiece 或 SentencePiece）。
     英文：直接按空格/标点划分，或基于子词的 BPE/WordPiece。
  b. 去停用词 (stopword removal)
     去除无实际意义的高频词（如“的”“and”）。
  c.词干化 vs 词形还原 (stemming vs lemmatization)
     Stemming：只做粗暴切割，例如“studying → study”（规则化）。
     Lemmatization：基于词典的标准化，还原成词典中的正规词。
  d.向量化方法 (Vectorization)
     Bag-of-Words：统计词频，简单但忽略语义。
     TF-IDF：考虑词在文档和语料中的区分度。
     Word2Vec / GloVe：把词映射到低维稠密向量，包含语义关系。
     Transformer Embeddings： contextualized 表示，不同语境下词向量不同（BERT）。
  e.序列建模相关
     Padding：序列对齐；短句补零以便批处理。
     Masking：在计算 self-attention 时避免模型看到 padding 部分或未来信息（语言建模）。

2. 应用 (Applications)
  a.文本分类
    新闻分类、垃圾邮件检测、评论情感极性分析。
  b.命名实体识别（NER, Named Entity Recognition）
    识别人名、地名、机构名等。
    常用于金融、医疗数据挖掘。
  c.问答与检索
    FAQ 问答，智能助手（Chatbot）。
    信息检索（搜索引擎文档匹配）。

--------------------------------------------------------------------------------------------
🎤 NLP 相关面试 Q&A 模拟
✅ Tokenization & Preprocessing
面试官： 为什么 NLP 模型中需要分词？

你：自然语言是离散符号序列，模型需要把它转化成计算机能处理的数字表示。分词/tokenization 是把原始文本拆解成词或子词单元。例如中文必须分词，英文可以用空格，但现在主流方法是 BPE 或 WordPiece，将词进一步分解为稳定的子词单元，减少 OOV（未登录词）。

✅ 向量化
面试官： TF-IDF 和 Word2Vec 的区别是什么？

你：
TF-IDF：基于词频统计，强调区分度，但向量维度高，且没有语义信息。
Word2Vec：基于神经网络训练得到稠密词向量，能捕捉语义关系（如 king - man ≈ queen - woman），效果更自然。
现在常用 Transformer Embeddings（如 BERT），能动态根据上下文生成词向量，比静态嵌入更强。

✅ Padding & Masking
面试官： NLP 模型中为什么要用 padding 和 masking？

你：
padding：因为不同句子长度不一样，需要补齐到统一长度，方便 batch 处理。
masking：
避免模型在注意力计算时关注 padding 部分；
在语言建模中，mask 可防止模型看到未来的词，用于保持因果性。


✅ 应用场景
面试官： NER（命名实体识别）和文本分类有什么区别？

你：
文本分类：判断一句话/一篇文档属于哪一类别，比如“积极/消极”、“体育/娱乐”。适合全局任务。
NER：从句子中抽取特定类型的词或短语，比如“乔布斯”（人名）、“苹果公司”（机构名）。更细粒度，需要逐词标注。


✅ 进阶问题
面试官： 传统 NLP 方法和基于深度学习的方法有什么不同？

你：
传统方法：依赖人工特征，如 TF-IDF、n-gram，模型一般是 SVM、LR。
深度学习方法：像 CNN、RNN、Transformer，可以自动学习特征表示，效果更好，尤其是在大规模语料上。Transformer 已经成为主流。
