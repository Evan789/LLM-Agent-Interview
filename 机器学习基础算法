📘 机器学习基础 — 面试资料
1. 模型比较

Logistic Regression
特点：线性模型，适合二分类任务。
优点：简单、可解释性强（系数直接反映特征影响方向与大小）。
缺点：只能处理线性关系，非线性拟合能力差。
应用场景：金融风控、广告点击率预测、医学二分类。
-------------------------------------------------------------
Random Forest
特点：集成学习（Bagging），由多棵决策树组成。
优点：可以处理非线性关系，泛化能力强，对特征交互敏感，抗过拟合。
缺点：模型庞大，可解释性差；预测速度较慢。
应用场景：结构化数据分类、特征维度较多的任务。
-------------------------------------------------------------
XGBoost
特点：基于 Boosting，迭代训练弱学习器（CART树），综合强大。
优点：精度高、鲁棒性好（处理缺失值能力强）、常用于比赛。
缺点：较复杂，调参成本高，训练时间长，需更多算力。
应用场景：Kaggle、推荐系统、CTR预测、金融/风控。

-------------------------------------------------------------
模型比较总结
可解释性最强 → Logistic Regression。
非线性建模好 → Random Forest。
精度最高（工业落地多） → XGBoost / LightGBM。
面试关键词：精度 vs 可解释性 vs 计算成本。

-------------------------------------------------------------
2. 特征工程
缺失值处理
均值/中位数/众数填补。
特殊值填补（如 -999）。
使用模型预测填补。

标准化与归一化
StandardScaler：减均值除以方差（适合正态分布特征）。
MinMaxScaler：线性缩放到 [0,1]（适合稀疏数据、稳定范围）。

类别特征处理
One-hot：适合无序类别。
Target encoding：适合高基数类别，依据目标变量进行编码。

特征选择
Filter：方差筛选、卡方检验、相关系数。
Wrapper：递归特征消除（RFE）。
Embedded：模型自带的特征选择（Lasso/L1正则化、树模型特征重要性）。

-----------------------------------------------------------------------------
🎤 模拟面试脚本（机器学习基础）
✅ 模型选择比较
面试官： Logistic Regression、Random Forest 和 XGBoost 各有什么优缺点？
你：

Logistic 回归：可解释性强，能直接解释特征对结果的影响，但对非线性问题无能为力。
Random Forest：非线性建模能力强，抗过拟合，适合多数实际场景，但可解释性差。
XGBoost：Boosting 思想，精度通常最高，适合竞赛和工业场景，但调参复杂、训练成本高。
在实际业务中，我会根据任务需求选择，如果需要可解释性 → Logistic；强大建模能力 → RF/XGB；追求极致精度 → 首选 XGBoost。

---------------------------------------------------------------------------------------------------------------
✅ 业务问题导向
面试官： 如果我要求既要有高精度，又要有一定可解释性，你会选用哪个？
你：
这要看数据特性：
如果数据维度多、特征非线性强，我会选 XGBoost，然后通过 SHAP、特征重要性等方法增强解释性。
如果业务场景对决策可解释性要求极高（比如金融风控），我可能会先用 Logistic，再结合衍生变量构造来提升模型能力。
----------------------------------------------------------------------------------------------------------------
✅ 特征工程
面试官： 你怎么处理缺失值？

你：常见方法有：
数值型：均值/中位数填补，或模型预测填补。
类别型：众数填补，或者加一个 “缺失” 类别。
如果缺失本身携带信息，可以引入缺失标记特征。
----------------------------------------------------------------------------
面试官： 为什么要做标准化或归一化？

你：
标准化：让特征均值为 0、方差为 1，避免特征量纲不同导致权重倾斜，适合正态分布特征。
归一化：将特征缩放到 [0,1]，适合数据稀疏、范围有限的场景。
另外，对于使用梯度下降的模型（如 LR、SVM、神经网络），标准化能加快收敛速度。
-----------------------------------------------------------------------------
面试官： 类别特征你怎么处理？什么时候选 one-hot，什么时候选 target encoding？

你：One-hot：适合低基数的无序类别，比如“性别”。
Target encoding：适合高基数类别，比如“用户ID、邮编”，能减少维度膨胀。
不过 Target encoding 容易引入泄漏，我会用交叉验证或平滑处理来避免。
----------------------------------------------------------------------------
面试官： 特征选择方法你能介绍一下吗？

你：可以分成三类：
Filter 型：先验指标过滤，比如卡方检验、皮尔逊相关、方差阈值。
Wrapper 型：递归特征消除（RFE），依靠模型迭代训练筛选特征，仅适合小规模数据。
Embedded 型：模型自带特征选择功能，比如 Lasso（L1 正则）或树模型的重要性评分。
实际应用中，通常先用 Filter 做粗筛，再用 Embedded 做精筛。
