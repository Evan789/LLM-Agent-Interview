📌 第一问：
面试官： 你能介绍一下深度学习中有哪些经典算法吗？

你（推荐回答框架）：
深度学习发展过程中，有几类经典算法/架构，奠定了整个领域的基础：

CNN（卷积神经网络） → 主要用于图像和空间特征建模，比如分类/检测。
RNN 系列（RNN, LSTM, GRU） → 处理序列数据，尤其是时间序列和自然语言。
自编码器/变分自编码器（Autoencoder/VAE） → 无监督特征学习和生成建模。
GAN（生成对抗网络） → 图像生成、风格迁移。
Transformer 系列（BERT, GPT, ViT） → 自注意力机制，在 NLP 和 CV 都是主流架构。
GNN（图神经网络） → 用于图结构数据，像社交网络、推荐系统。
小技巧：开头先列清楚类别，表现知识面，再根据面试官追问展开。

----------------------------------------------------------------------------------------------------

第二问：
面试官： CNN 为什么适合做图像任务？

你：CNN 的核心是 卷积操作 和 权值共享。

卷积能提取图像的局部特征（边缘、纹理），逐层叠加形成抽象语义特征。
权值共享减少参数量，提高运算效率。
池化层提供平移不变性，使模型对位置 perturbation 鲁棒。
所以 CNN 特别适合图像分类、检测、分割这类任务，是计算机视觉的基础。

-----------------------------------------------------------------------------------------------------

📌 第三问：
面试官： RNN 为何会被 LSTM、Transformer 替代？

你：
RNN 在长序列中存在 梯度消失/爆炸，难以捕获长距离依赖。

LSTM 引入“门控机制”，能缓解梯度消失，使长依赖建模更好。
Transformer 出现后，利用 Self-Attention 并行建模序列，能一次性捕获全局依赖，效率和效果都远超 RNN。
目前序列任务几乎被 Transformer 系列模型取代。

-----------------------------------------------------------------------------------------------------

📌 第四问：
面试官： GAN 的基本思想是什么？最大的难点是什么？

你：
GAN 由 生成器（G） 和 判别器（D） 组成：

G 负责生成尽可能“以假乱真”的样本
D 负责区分真样本和伪样本
两者通过对抗训练，最后 G 学到真实数据分布。
最大难点是 训练不稳定：

经常出现模式坍缩（Generator 只学会生成少数几种样本）。
损失函数高度非凸，需要很多调参技巧（比如 Wasserstein GAN，谱归一化）。

--------------------------------------------------------------------------------------------------

📌 第五问：
面试官： Transformer 为什么会成为 NLP 和多模态任务的主流？

你：
核心原因是 Self-Attention：

能直接捕获全局依赖关系，比 RNN 更容易建模长文本。
计算时可并行，利用 GPU 训练速度更快。
结合大规模数据和预训练（如 BERT、GPT），效果远优于传统模型。
同时，Transformer 还扩展到 CV（ViT）、多模态（CLIP）等领域，几乎成为深度学习的统一架构。

--------------------------------------------------------------------------------------------------

📌 第六问：
面试官： 除了以上模型，你能补充一些比较新的发展趋势吗？

你：

Diffusion Models（扩散模型）：近年来超越 GAN 的生成模型，在图像合成领域（如 Stable Diffusion、DALL·E）表现极佳。
大语言模型（LLM, Large Language Models）：基于 Transformer + 海量训练数据，具备强大的推理和生成能力（如 GPT 系列）。
多模态模型：如 CLIP、GPT-4V，将文本、图像、语音等融合建模。
（如果能多提新趋势，面试官会觉得你跟得上前沿 😎）

🔑 答题小技巧
答题时分层次：先类比总结 → 再展开一个例子。
关键词：
CNN = 局部特征 + 权值共享
RNN = 隐状态，缺点梯度消失
LSTM/GRU = 门控机制
GAN = 对抗训练 + 生成图像
Transformer = Self-Attention + 并行
GNN = 图结构建模

