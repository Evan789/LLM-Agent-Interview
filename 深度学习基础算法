✅ LSTM
LSTM（长短期记忆网络）
基础知识
结构：由三个“门”控制信息流动：

输入门 (input gate)：决定当前输入多少被保存到记忆单元。
遗忘门 (forget gate)：决定之前的记忆有多少被保留/丢弃。
输出门 (output gate)：决定记忆单元的状态如何输出。
优势：相比传统 RNN，能够缓解梯度消失问题，能捕获长距离依赖。

应用场景：
时间序列预测（股票价格、传感器数据）
文本建模（情感分析）
语音处理（语音识别、语音合成）

面试官： LSTM 相比普通 RNN 有什么改进？
你：RNN 在长序列训练时容易出现梯度消失或爆炸，学不到长距离依赖。LSTM 通过引入输入门、遗忘门、输出门，能控制信息的保留和丢弃，从而在梯度传播时更稳定，能更好建模长序列。

面试官： LSTM 在实际业务中有哪些应用？
你：时间序列预测，如金融交易量预测；
NLP，如情感分类、机器翻译（早期翻译模型）；
语音任务，如语音识别。

---------------------------------------------------------------------------------------------------------

✅ Transformer

核心架构
Encoder-Decoder：常用于序列到序列任务。
Self-Attention：能捕获序列中任意两个位置的依赖关系。
Multi-Head Attention：多个子空间并行学习不同关注模式。
Positional Encoding：引入序列顺序信息（因为 Transformer 结构本身不具备顺序感知）。

相比 LSTM 的优势：
可并行训练，不依赖时间步迭代，训练效率高。
能捕获长距离依赖，避免 LSTM 在极长序列下的性能不足。
在大规模数据上更具优势，成为 NLP 和 CV 主流。

应用
NLP: BERT, GPT, T5
CV: ViT (Vision Transformer)
多模态建模：CLIP, Flamingo 等

面试官： Transformer 为什么比 LSTM 更适合长文本？
你：因为 Transformer 使用 Self-Attention 机制，计算时每个位置都能直接关注到序列中的任意位置，能高效捕获长距离依赖。而 LSTM 在长序列下依赖逐步传递，距离越远信息衰减越大。

面试官： Transformer 为什么能加速训练？
你：LSTM 是序列依赖的，要按时间步一个个计算；Transformer 使用矩阵运算处理整个序列，可以并行化，利用 GPU/TPU 高效训练。

面试官： 为什么需要 Positional Encoding？
你：Attention 机制本身与位置无关，序列输入经过 Attention 会丢失顺序信息。Positional Encoding 通过加入位置信息向量，让模型知道 “第n个词” 的相对或绝对位置信息，从而理解序列顺序。

-----------------------------------------------------------------------------------------------------------

深度学习经典几大算法（模型）
1. 卷积神经网络（CNN, Convolutional Neural Network）
✨ 原理
通过 卷积操作（局部感受野 + 权值共享） 来提取局部特征，逐层叠加得到更抽象的特征表示；通常与 **池化（Pooling）**层结合以降低维度。

🔧 代表模型
LeNet (1998)：最早用于手写数字识别（MNIST）。
AlexNet (2012)：ImageNet 冠军，推动了 GPU+深度学习的浪潮。
VGG, ResNet (2015)：更深层次的 CNN，引入残差结构。
🛠 应用
图像分类、人脸识别、医学影像分析、目标检测（YOLO、Faster R-CNN）、推荐系统（学习用户-物品交互特征）。

👍 优缺点
优点：自动特征提取，效果远超传统手工特征。
缺点：对空间位置不变性好，但难以处理序列依赖。

------------------------------------------------------------------------------------------------------------------------
2. 循环神经网络（RNN, Recurrent Neural Network）
✨ 原理
通过 隐状态（hidden state） 递归传递，使模型能够记忆过去的输入，适用于处理序列数据。

🔧 改进模型
RNN 基础模型：容易梯度消失/爆炸，难以学习长依赖。
LSTM（1997）：引入输入门、遗忘门、输出门，缓解梯度消失。适合长序列。
GRU（2014）：更轻量的 LSTM（只有更新门和重置门）。
🛠 应用
语音识别、机器翻译（早期）、时间序列预测。

👍 优缺点
优点：天然适合序列数据建模。
缺点：训练慢，长依赖捕捉仍有限。

------------------------------------------------------------------------------------------------------------------------
3. 自编码器（Autoencoder）
✨ 原理
通过 编码器 → 解码器 结构学习输入数据的低维表示，再解码重构。目标是让重建误差最小化。

🔧 变体
Denoising Autoencoder：在输入加噪音，逼迫模型学习鲁棒特征。
Variational Autoencoder (VAE)：引入概率推断框架，用于生成任务。
🛠 应用
降维、特征学习、图像去噪、生成模型（图像生成/推荐系统 embedding 学习）。

👍 优缺点
优点：无监督学习，不依赖标签。
缺点：生成效果不如 GAN / Diffusion。

-------------------------------------------------------------------------------------------------------------------------
4. 生成对抗网络（GAN, Generative Adversarial Network）
✨ 原理
包含 生成器（Generator, G） 和 判别器（Discriminator, D），通过对抗训练学习数据分布。G 生成伪造样本，D 试图区分真假，二者交替优化。

🔧 代表模型
DCGAN（图像生成）
CycleGAN（图像风格迁移）
StyleGAN（高质量人脸生成）
🛠 应用
图像生成、风格迁移、超分辨率、数据增强。

👍 优缺点
优点：生成质量高，非常灵活。
缺点：训练不稳定，容易模式坍缩（collapse）。

--------------------------------------------------------------------------------------------------------------------------
5. Transformer 系列
✨ 原理
基于 Self-Attention 机制，突破 RNN/CNN 在长序列上的局限。
核心：序列中每个元素可直接与其他元素关联，并行计算。

🔧 代表模型
原始 Transformer (2017)：Seq2Seq + Attention。
BERT (2018)：双向编码预训练模型。
GPT 系列：解码器架构，生成式预训练巨模型。
ViT (2020+)：把图像切 patch，当作序列送入 Transformer。
🛠 应用
NLP（翻译、问答、对话、文本生成）。
CV（图像分类、检测）。
多模态（CLIP, Flamingo, Sora）。
👍 优缺点
优点：长依赖建模 + 并行性 + 在大规模数据上效果极佳。
缺点：参数量庞大，训练需要极高算力。

----------------------------------------------------------------------------------------------------------------------------------
6. 图神经网络（GNN, Graph Neural Network）
✨ 原理
通过 消息传递（message passing） 让节点聚合邻居信息，学习图结构数据的表示。

🔧 代表方法
GCN（Graph Convolutional Network）
GAT（Graph Attention Network）
🛠 应用
社交网络建模、推荐系统（用户-物品图）、分子性质预测。

👍 优缺点
优点：天生适合图结构数据。
缺点：计算代价高，难以扩展到大规模图。

-----------------------------------------------------------------------------------------------------------------------------------
🎯 总结表（面试速记）
算法/架构	           关键思想	         代表模型	          应用	           缺点
CNN	              卷积+权值共享	     LeNet, ResNet	   图像识别、检测	   难处理序列
RNN/LSTM/GRU	    递归+门机制	       LSTM, GRU	       NLP、时间序列	     训练慢、长依赖差
Autoencoder/VAE	  压缩-解码	         VAE	             降维、生成	       生成质量一般
GAN	              对抗博弈	           DCGAN, StyleGAN	 图像生成	         训练不稳定
Transformer	      Self-Attention	   BERT, GPT, ViT	   NLP、CV、多模态	   算力消耗大
GNN	              图卷积	             GCN, GAT	         推荐、分子预测	   大规模困难
